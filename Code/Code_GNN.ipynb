{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deterministic(random_seed = 6):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "make_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pd.read_csv('../node_information.csv', header=None)\n",
    "train = pd.read_csv('../train.txt', header=None, names=['Source','Target','Edge'], delim_whitespace=True)\n",
    "test = pd.read_csv('../test.txt', header=None, names=['Source','Target'], delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>923</th>\n",
       "      <th>924</th>\n",
       "      <th>925</th>\n",
       "      <th>926</th>\n",
       "      <th>927</th>\n",
       "      <th>928</th>\n",
       "      <th>929</th>\n",
       "      <th>930</th>\n",
       "      <th>931</th>\n",
       "      <th>932</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 933 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  923  924  925  926  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    4    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    5    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    6    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    7    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   927  928  929  930  931  932  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 933 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.loc[:, node.columns != node.columns[0]] = node.loc[:, node.columns != node.columns[0]].astype(int)\n",
    "node.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 5, 6, 7, 9, 10, 11, 13, 17]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = [i for i in node[0]]\n",
    "nodes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_index</th>\n",
       "      <th>new_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>7584</td>\n",
       "      <td>3592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>7589</td>\n",
       "      <td>3593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>7593</td>\n",
       "      <td>3594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>7594</td>\n",
       "      <td>3595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>7599</td>\n",
       "      <td>3596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3597 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      old_index  new_index\n",
       "0             0          0\n",
       "1             4          1\n",
       "2             5          2\n",
       "3             6          3\n",
       "4             7          4\n",
       "...         ...        ...\n",
       "3592       7584       3592\n",
       "3593       7589       3593\n",
       "3594       7593       3594\n",
       "3595       7594       3595\n",
       "3596       7599       3596\n",
       "\n",
       "[3597 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_idx = pd.DataFrame({'old_index': nodes, 'new_index': range(3597)})\n",
    "node_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>939</td>\n",
       "      <td>3809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2442</td>\n",
       "      <td>5784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>3809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>857</td>\n",
       "      <td>2280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1358</td>\n",
       "      <td>5722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10480</th>\n",
       "      <td>147</td>\n",
       "      <td>4217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10482</th>\n",
       "      <td>2486</td>\n",
       "      <td>3809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10485</th>\n",
       "      <td>2702</td>\n",
       "      <td>5862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10490</th>\n",
       "      <td>1240</td>\n",
       "      <td>6164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10493</th>\n",
       "      <td>3341</td>\n",
       "      <td>6717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5248 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Source  Target  Edge\n",
       "0         939    3809     1\n",
       "1        2442    5784     1\n",
       "2         179    3809     1\n",
       "3         857    2280     1\n",
       "4        1358    5722     1\n",
       "...       ...     ...   ...\n",
       "10480     147    4217     1\n",
       "10482    2486    3809     1\n",
       "10485    2702    5862     1\n",
       "10490    1240    6164     1\n",
       "10493    3341    6717     1\n",
       "\n",
       "[5248 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tensor the edge_index\n",
    "train_edge = train[train['Edge'] == 1]\n",
    "train_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_new = []\n",
    "target_new = []\n",
    "for i in range(train_edge.shape[0]):\n",
    "    s_new = node_idx.loc[node_idx['old_index'] == np.array(train_edge['Source'])[i], 'new_index'].values[0]\n",
    "    t_new = node_idx.loc[node_idx['old_index'] == np.array(train_edge['Target'])[i], 'new_index'].values[0]\n",
    "    source_new.append(s_new)\n",
    "    target_new.append(t_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_new = torch.from_numpy(np.array(source_new))\n",
    "target_new = torch.from_numpy(np.array(target_new))\n",
    "edge_index = torch.stack([source_new, target_new], dim=0)\n",
    "assert edge_index.size() == (2,5248)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tensor for node_feature\n",
    "node_feat = torch.from_numpy(node.values[:,1:]).to(torch.float)\n",
    "assert node_feat.size() == (3597, 932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3597, 932], edge_index=[2, 5248])\n"
     ]
    }
   ],
   "source": [
    "# Create the graph by inserting the data and features directly\n",
    "G = Data(x=node_feat, edge_index=edge_index)\n",
    "print(G)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: Data(x=[3597, 932], edge_index=[2, 3936], edge_label=[7872], edge_label_index=[2, 7872])\n",
      "val_data: Data(x=[3597, 932], edge_index=[2, 3936], edge_label=[2624], edge_label_index=[2, 2624])\n"
     ]
    }
   ],
   "source": [
    "split = T.RandomLinkSplit(\n",
    "    num_val=0.25,\n",
    "    num_test=0,\n",
    "    is_undirected=False,\n",
    "    add_negative_train_samples=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "train_data, val_data, _ = split(G)\n",
    "print('train_data:', train_data)\n",
    "print('val_data:', val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 256)\n",
    "        self.conv2 = GCNConv(256, 128)\n",
    "        self.conv3 = GCNConv(128, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        return self.conv4(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(\n",
    "            dim=-1\n",
    "        )  # product of a pair of nodes on each edge\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "    \n",
    "\n",
    "def train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=200\n",
    "):\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x.to(device), train_data.edge_index.to(device))\n",
    "\n",
    "        # sampling training negatives for every training epoch\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "            num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        edge_label_index = torch.cat(\n",
    "            [train_data.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        ).to(device)\n",
    "        \n",
    "        edge_label = torch.cat([\n",
    "            train_data.edge_label,\n",
    "            train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0).to(device)\n",
    "\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_auc = eval_link_predictor(model, val_data)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "\n",
    "    model.eval()\n",
    "    z = model.encode(data.x.to(device), data.edge_index.to(device))\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Loss: 0.693, Val AUC: 0.523\n",
      "Epoch: 020, Train Loss: 0.692, Val AUC: 0.586\n",
      "Epoch: 030, Train Loss: 0.691, Val AUC: 0.606\n",
      "Epoch: 040, Train Loss: 0.689, Val AUC: 0.597\n",
      "Epoch: 050, Train Loss: 0.686, Val AUC: 0.581\n",
      "Epoch: 060, Train Loss: 0.680, Val AUC: 0.561\n",
      "Epoch: 070, Train Loss: 0.673, Val AUC: 0.548\n",
      "Epoch: 080, Train Loss: 0.666, Val AUC: 0.536\n",
      "Epoch: 090, Train Loss: 0.658, Val AUC: 0.536\n",
      "Epoch: 100, Train Loss: 0.650, Val AUC: 0.538\n",
      "Epoch: 110, Train Loss: 0.647, Val AUC: 0.539\n",
      "Epoch: 120, Train Loss: 0.643, Val AUC: 0.535\n",
      "Epoch: 130, Train Loss: 0.640, Val AUC: 0.538\n",
      "Epoch: 140, Train Loss: 0.637, Val AUC: 0.539\n",
      "Epoch: 150, Train Loss: 0.633, Val AUC: 0.540\n",
      "Epoch: 160, Train Loss: 0.630, Val AUC: 0.537\n",
      "Epoch: 170, Train Loss: 0.625, Val AUC: 0.532\n",
      "Epoch: 180, Train Loss: 0.628, Val AUC: 0.531\n",
      "Epoch: 190, Train Loss: 0.624, Val AUC: 0.527\n",
      "Epoch: 200, Train Loss: 0.621, Val AUC: 0.527\n",
      "Epoch: 210, Train Loss: 0.624, Val AUC: 0.525\n",
      "Epoch: 220, Train Loss: 0.620, Val AUC: 0.525\n",
      "Epoch: 230, Train Loss: 0.618, Val AUC: 0.523\n",
      "Epoch: 240, Train Loss: 0.617, Val AUC: 0.523\n",
      "Epoch: 250, Train Loss: 0.615, Val AUC: 0.522\n",
      "Epoch: 260, Train Loss: 0.613, Val AUC: 0.524\n",
      "Epoch: 270, Train Loss: 0.614, Val AUC: 0.523\n",
      "Epoch: 280, Train Loss: 0.610, Val AUC: 0.525\n",
      "Epoch: 290, Train Loss: 0.609, Val AUC: 0.525\n",
      "Epoch: 300, Train Loss: 0.611, Val AUC: 0.524\n",
      "Epoch: 310, Train Loss: 0.604, Val AUC: 0.523\n",
      "Epoch: 320, Train Loss: 0.605, Val AUC: 0.521\n",
      "Epoch: 330, Train Loss: 0.605, Val AUC: 0.522\n",
      "Epoch: 340, Train Loss: 0.603, Val AUC: 0.521\n",
      "Epoch: 350, Train Loss: 0.602, Val AUC: 0.522\n",
      "Epoch: 360, Train Loss: 0.607, Val AUC: 0.520\n",
      "Epoch: 370, Train Loss: 0.601, Val AUC: 0.522\n",
      "Epoch: 380, Train Loss: 0.603, Val AUC: 0.522\n",
      "Epoch: 390, Train Loss: 0.606, Val AUC: 0.520\n",
      "Epoch: 400, Train Loss: 0.598, Val AUC: 0.522\n",
      "Epoch: 410, Train Loss: 0.599, Val AUC: 0.520\n",
      "Epoch: 420, Train Loss: 0.600, Val AUC: 0.519\n",
      "Epoch: 430, Train Loss: 0.601, Val AUC: 0.519\n",
      "Epoch: 440, Train Loss: 0.600, Val AUC: 0.521\n",
      "Epoch: 450, Train Loss: 0.598, Val AUC: 0.520\n",
      "Epoch: 460, Train Loss: 0.597, Val AUC: 0.520\n",
      "Epoch: 470, Train Loss: 0.596, Val AUC: 0.522\n",
      "Epoch: 480, Train Loss: 0.595, Val AUC: 0.519\n",
      "Epoch: 490, Train Loss: 0.594, Val AUC: 0.520\n",
      "Epoch: 500, Train Loss: 0.595, Val AUC: 0.518\n",
      "Epoch: 510, Train Loss: 0.593, Val AUC: 0.520\n",
      "Epoch: 520, Train Loss: 0.590, Val AUC: 0.518\n",
      "Epoch: 530, Train Loss: 0.594, Val AUC: 0.521\n",
      "Epoch: 540, Train Loss: 0.596, Val AUC: 0.521\n",
      "Epoch: 550, Train Loss: 0.591, Val AUC: 0.521\n",
      "Epoch: 560, Train Loss: 0.590, Val AUC: 0.521\n",
      "Epoch: 570, Train Loss: 0.587, Val AUC: 0.521\n",
      "Epoch: 580, Train Loss: 0.588, Val AUC: 0.519\n",
      "Epoch: 590, Train Loss: 0.591, Val AUC: 0.519\n",
      "Epoch: 600, Train Loss: 0.591, Val AUC: 0.518\n",
      "Epoch: 610, Train Loss: 0.593, Val AUC: 0.517\n",
      "Epoch: 620, Train Loss: 0.592, Val AUC: 0.518\n",
      "Epoch: 630, Train Loss: 0.593, Val AUC: 0.519\n",
      "Epoch: 640, Train Loss: 0.587, Val AUC: 0.521\n",
      "Epoch: 650, Train Loss: 0.593, Val AUC: 0.516\n",
      "Epoch: 660, Train Loss: 0.593, Val AUC: 0.519\n",
      "Epoch: 670, Train Loss: 0.587, Val AUC: 0.518\n",
      "Epoch: 680, Train Loss: 0.594, Val AUC: 0.519\n",
      "Epoch: 690, Train Loss: 0.590, Val AUC: 0.519\n",
      "Epoch: 700, Train Loss: 0.591, Val AUC: 0.516\n",
      "Epoch: 710, Train Loss: 0.586, Val AUC: 0.521\n",
      "Epoch: 720, Train Loss: 0.590, Val AUC: 0.518\n",
      "Epoch: 730, Train Loss: 0.584, Val AUC: 0.522\n",
      "Epoch: 740, Train Loss: 0.588, Val AUC: 0.522\n",
      "Epoch: 750, Train Loss: 0.585, Val AUC: 0.522\n",
      "Epoch: 760, Train Loss: 0.590, Val AUC: 0.521\n",
      "Epoch: 770, Train Loss: 0.589, Val AUC: 0.523\n",
      "Epoch: 780, Train Loss: 0.586, Val AUC: 0.521\n",
      "Epoch: 790, Train Loss: 0.586, Val AUC: 0.524\n",
      "Epoch: 800, Train Loss: 0.586, Val AUC: 0.522\n",
      "Epoch: 810, Train Loss: 0.588, Val AUC: 0.522\n",
      "Epoch: 820, Train Loss: 0.586, Val AUC: 0.523\n",
      "Epoch: 830, Train Loss: 0.588, Val AUC: 0.523\n",
      "Epoch: 840, Train Loss: 0.586, Val AUC: 0.518\n",
      "Epoch: 850, Train Loss: 0.582, Val AUC: 0.522\n",
      "Epoch: 860, Train Loss: 0.585, Val AUC: 0.522\n",
      "Epoch: 870, Train Loss: 0.587, Val AUC: 0.521\n",
      "Epoch: 880, Train Loss: 0.584, Val AUC: 0.525\n",
      "Epoch: 890, Train Loss: 0.582, Val AUC: 0.521\n",
      "Epoch: 900, Train Loss: 0.581, Val AUC: 0.522\n",
      "Epoch: 910, Train Loss: 0.580, Val AUC: 0.522\n",
      "Epoch: 920, Train Loss: 0.583, Val AUC: 0.521\n",
      "Epoch: 930, Train Loss: 0.586, Val AUC: 0.522\n",
      "Epoch: 940, Train Loss: 0.585, Val AUC: 0.521\n",
      "Epoch: 950, Train Loss: 0.584, Val AUC: 0.521\n",
      "Epoch: 960, Train Loss: 0.581, Val AUC: 0.523\n",
      "Epoch: 970, Train Loss: 0.586, Val AUC: 0.520\n",
      "Epoch: 980, Train Loss: 0.582, Val AUC: 0.522\n",
      "Epoch: 990, Train Loss: 0.576, Val AUC: 0.524\n",
      "Epoch: 1000, Train Loss: 0.583, Val AUC: 0.522\n",
      "Epoch: 1010, Train Loss: 0.582, Val AUC: 0.526\n",
      "Epoch: 1020, Train Loss: 0.583, Val AUC: 0.521\n",
      "Epoch: 1030, Train Loss: 0.583, Val AUC: 0.522\n",
      "Epoch: 1040, Train Loss: 0.581, Val AUC: 0.523\n",
      "Epoch: 1050, Train Loss: 0.580, Val AUC: 0.522\n",
      "Epoch: 1060, Train Loss: 0.584, Val AUC: 0.525\n",
      "Epoch: 1070, Train Loss: 0.579, Val AUC: 0.526\n",
      "Epoch: 1080, Train Loss: 0.581, Val AUC: 0.524\n",
      "Epoch: 1090, Train Loss: 0.582, Val AUC: 0.525\n",
      "Epoch: 1100, Train Loss: 0.580, Val AUC: 0.525\n",
      "Epoch: 1110, Train Loss: 0.581, Val AUC: 0.525\n",
      "Epoch: 1120, Train Loss: 0.583, Val AUC: 0.522\n",
      "Epoch: 1130, Train Loss: 0.584, Val AUC: 0.525\n",
      "Epoch: 1140, Train Loss: 0.577, Val AUC: 0.526\n",
      "Epoch: 1150, Train Loss: 0.585, Val AUC: 0.525\n",
      "Epoch: 1160, Train Loss: 0.581, Val AUC: 0.526\n",
      "Epoch: 1170, Train Loss: 0.577, Val AUC: 0.523\n",
      "Epoch: 1180, Train Loss: 0.576, Val AUC: 0.526\n",
      "Epoch: 1190, Train Loss: 0.580, Val AUC: 0.525\n",
      "Epoch: 1200, Train Loss: 0.580, Val AUC: 0.526\n",
      "Epoch: 1210, Train Loss: 0.583, Val AUC: 0.524\n",
      "Epoch: 1220, Train Loss: 0.581, Val AUC: 0.524\n",
      "Epoch: 1230, Train Loss: 0.582, Val AUC: 0.524\n",
      "Epoch: 1240, Train Loss: 0.575, Val AUC: 0.525\n",
      "Epoch: 1250, Train Loss: 0.578, Val AUC: 0.525\n",
      "Epoch: 1260, Train Loss: 0.581, Val AUC: 0.527\n",
      "Epoch: 1270, Train Loss: 0.576, Val AUC: 0.526\n",
      "Epoch: 1280, Train Loss: 0.580, Val AUC: 0.525\n",
      "Epoch: 1290, Train Loss: 0.583, Val AUC: 0.525\n",
      "Epoch: 1300, Train Loss: 0.575, Val AUC: 0.525\n",
      "Epoch: 1310, Train Loss: 0.581, Val AUC: 0.526\n",
      "Epoch: 1320, Train Loss: 0.576, Val AUC: 0.525\n",
      "Epoch: 1330, Train Loss: 0.578, Val AUC: 0.526\n",
      "Epoch: 1340, Train Loss: 0.579, Val AUC: 0.527\n",
      "Epoch: 1350, Train Loss: 0.578, Val AUC: 0.525\n",
      "Epoch: 1360, Train Loss: 0.574, Val AUC: 0.527\n",
      "Epoch: 1370, Train Loss: 0.580, Val AUC: 0.526\n",
      "Epoch: 1380, Train Loss: 0.578, Val AUC: 0.528\n",
      "Epoch: 1390, Train Loss: 0.576, Val AUC: 0.525\n",
      "Epoch: 1400, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 1410, Train Loss: 0.576, Val AUC: 0.526\n",
      "Epoch: 1420, Train Loss: 0.585, Val AUC: 0.527\n",
      "Epoch: 1430, Train Loss: 0.581, Val AUC: 0.527\n",
      "Epoch: 1440, Train Loss: 0.579, Val AUC: 0.527\n",
      "Epoch: 1450, Train Loss: 0.576, Val AUC: 0.526\n",
      "Epoch: 1460, Train Loss: 0.579, Val AUC: 0.527\n",
      "Epoch: 1470, Train Loss: 0.580, Val AUC: 0.526\n",
      "Epoch: 1480, Train Loss: 0.580, Val AUC: 0.528\n",
      "Epoch: 1490, Train Loss: 0.580, Val AUC: 0.527\n",
      "Epoch: 1500, Train Loss: 0.574, Val AUC: 0.527\n",
      "Epoch: 1510, Train Loss: 0.580, Val AUC: 0.527\n",
      "Epoch: 1520, Train Loss: 0.577, Val AUC: 0.526\n",
      "Epoch: 1530, Train Loss: 0.575, Val AUC: 0.525\n",
      "Epoch: 1540, Train Loss: 0.576, Val AUC: 0.526\n",
      "Epoch: 1550, Train Loss: 0.575, Val AUC: 0.527\n",
      "Epoch: 1560, Train Loss: 0.578, Val AUC: 0.529\n",
      "Epoch: 1570, Train Loss: 0.580, Val AUC: 0.528\n",
      "Epoch: 1580, Train Loss: 0.580, Val AUC: 0.525\n",
      "Epoch: 1590, Train Loss: 0.573, Val AUC: 0.525\n",
      "Epoch: 1600, Train Loss: 0.574, Val AUC: 0.527\n",
      "Epoch: 1610, Train Loss: 0.579, Val AUC: 0.527\n",
      "Epoch: 1620, Train Loss: 0.575, Val AUC: 0.526\n",
      "Epoch: 1630, Train Loss: 0.578, Val AUC: 0.528\n",
      "Epoch: 1640, Train Loss: 0.574, Val AUC: 0.527\n",
      "Epoch: 1650, Train Loss: 0.578, Val AUC: 0.526\n",
      "Epoch: 1660, Train Loss: 0.570, Val AUC: 0.527\n",
      "Epoch: 1670, Train Loss: 0.578, Val AUC: 0.527\n",
      "Epoch: 1680, Train Loss: 0.576, Val AUC: 0.527\n",
      "Epoch: 1690, Train Loss: 0.573, Val AUC: 0.529\n",
      "Epoch: 1700, Train Loss: 0.581, Val AUC: 0.528\n",
      "Epoch: 1710, Train Loss: 0.576, Val AUC: 0.527\n",
      "Epoch: 1720, Train Loss: 0.577, Val AUC: 0.528\n",
      "Epoch: 1730, Train Loss: 0.576, Val AUC: 0.528\n",
      "Epoch: 1740, Train Loss: 0.576, Val AUC: 0.530\n",
      "Epoch: 1750, Train Loss: 0.577, Val AUC: 0.530\n",
      "Epoch: 1760, Train Loss: 0.572, Val AUC: 0.530\n",
      "Epoch: 1770, Train Loss: 0.577, Val AUC: 0.530\n",
      "Epoch: 1780, Train Loss: 0.574, Val AUC: 0.528\n",
      "Epoch: 1790, Train Loss: 0.575, Val AUC: 0.530\n",
      "Epoch: 1800, Train Loss: 0.575, Val AUC: 0.529\n",
      "Epoch: 1810, Train Loss: 0.577, Val AUC: 0.529\n",
      "Epoch: 1820, Train Loss: 0.577, Val AUC: 0.528\n",
      "Epoch: 1830, Train Loss: 0.574, Val AUC: 0.530\n",
      "Epoch: 1840, Train Loss: 0.581, Val AUC: 0.528\n",
      "Epoch: 1850, Train Loss: 0.576, Val AUC: 0.530\n",
      "Epoch: 1860, Train Loss: 0.575, Val AUC: 0.531\n",
      "Epoch: 1870, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 1880, Train Loss: 0.577, Val AUC: 0.531\n",
      "Epoch: 1890, Train Loss: 0.579, Val AUC: 0.531\n",
      "Epoch: 1900, Train Loss: 0.576, Val AUC: 0.530\n",
      "Epoch: 1910, Train Loss: 0.573, Val AUC: 0.531\n",
      "Epoch: 1920, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 1930, Train Loss: 0.576, Val AUC: 0.530\n",
      "Epoch: 1940, Train Loss: 0.574, Val AUC: 0.530\n",
      "Epoch: 1950, Train Loss: 0.572, Val AUC: 0.529\n",
      "Epoch: 1960, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 1970, Train Loss: 0.569, Val AUC: 0.531\n",
      "Epoch: 1980, Train Loss: 0.575, Val AUC: 0.530\n",
      "Epoch: 1990, Train Loss: 0.577, Val AUC: 0.531\n",
      "Epoch: 2000, Train Loss: 0.577, Val AUC: 0.531\n",
      "Epoch: 2010, Train Loss: 0.579, Val AUC: 0.532\n",
      "Epoch: 2020, Train Loss: 0.573, Val AUC: 0.532\n",
      "Epoch: 2030, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 2040, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 2050, Train Loss: 0.577, Val AUC: 0.532\n",
      "Epoch: 2060, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 2070, Train Loss: 0.577, Val AUC: 0.531\n",
      "Epoch: 2080, Train Loss: 0.574, Val AUC: 0.530\n",
      "Epoch: 2090, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 2100, Train Loss: 0.570, Val AUC: 0.531\n",
      "Epoch: 2110, Train Loss: 0.575, Val AUC: 0.531\n",
      "Epoch: 2120, Train Loss: 0.572, Val AUC: 0.528\n",
      "Epoch: 2130, Train Loss: 0.574, Val AUC: 0.533\n",
      "Epoch: 2140, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 2150, Train Loss: 0.575, Val AUC: 0.530\n",
      "Epoch: 2160, Train Loss: 0.577, Val AUC: 0.534\n",
      "Epoch: 2170, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2180, Train Loss: 0.575, Val AUC: 0.536\n",
      "Epoch: 2190, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 2200, Train Loss: 0.576, Val AUC: 0.533\n",
      "Epoch: 2210, Train Loss: 0.577, Val AUC: 0.532\n",
      "Epoch: 2220, Train Loss: 0.568, Val AUC: 0.532\n",
      "Epoch: 2230, Train Loss: 0.575, Val AUC: 0.531\n",
      "Epoch: 2240, Train Loss: 0.576, Val AUC: 0.531\n",
      "Epoch: 2250, Train Loss: 0.571, Val AUC: 0.532\n",
      "Epoch: 2260, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2270, Train Loss: 0.578, Val AUC: 0.532\n",
      "Epoch: 2280, Train Loss: 0.571, Val AUC: 0.531\n",
      "Epoch: 2290, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2300, Train Loss: 0.575, Val AUC: 0.534\n",
      "Epoch: 2310, Train Loss: 0.574, Val AUC: 0.532\n",
      "Epoch: 2320, Train Loss: 0.574, Val AUC: 0.534\n",
      "Epoch: 2330, Train Loss: 0.578, Val AUC: 0.529\n",
      "Epoch: 2340, Train Loss: 0.576, Val AUC: 0.532\n",
      "Epoch: 2350, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 2360, Train Loss: 0.575, Val AUC: 0.531\n",
      "Epoch: 2370, Train Loss: 0.575, Val AUC: 0.535\n",
      "Epoch: 2380, Train Loss: 0.571, Val AUC: 0.534\n",
      "Epoch: 2390, Train Loss: 0.571, Val AUC: 0.532\n",
      "Epoch: 2400, Train Loss: 0.572, Val AUC: 0.534\n",
      "Epoch: 2410, Train Loss: 0.572, Val AUC: 0.533\n",
      "Epoch: 2420, Train Loss: 0.575, Val AUC: 0.533\n",
      "Epoch: 2430, Train Loss: 0.577, Val AUC: 0.532\n",
      "Epoch: 2440, Train Loss: 0.574, Val AUC: 0.533\n",
      "Epoch: 2450, Train Loss: 0.569, Val AUC: 0.530\n",
      "Epoch: 2460, Train Loss: 0.573, Val AUC: 0.532\n",
      "Epoch: 2470, Train Loss: 0.576, Val AUC: 0.531\n",
      "Epoch: 2480, Train Loss: 0.574, Val AUC: 0.532\n",
      "Epoch: 2490, Train Loss: 0.573, Val AUC: 0.533\n",
      "Epoch: 2500, Train Loss: 0.576, Val AUC: 0.532\n",
      "Epoch: 2510, Train Loss: 0.575, Val AUC: 0.531\n",
      "Epoch: 2520, Train Loss: 0.573, Val AUC: 0.529\n",
      "Epoch: 2530, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 2540, Train Loss: 0.568, Val AUC: 0.531\n",
      "Epoch: 2550, Train Loss: 0.571, Val AUC: 0.530\n",
      "Epoch: 2560, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 2570, Train Loss: 0.569, Val AUC: 0.531\n",
      "Epoch: 2580, Train Loss: 0.575, Val AUC: 0.532\n",
      "Epoch: 2590, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 2600, Train Loss: 0.571, Val AUC: 0.535\n",
      "Epoch: 2610, Train Loss: 0.568, Val AUC: 0.532\n",
      "Epoch: 2620, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 2630, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 2640, Train Loss: 0.574, Val AUC: 0.533\n",
      "Epoch: 2650, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 2660, Train Loss: 0.564, Val AUC: 0.529\n",
      "Epoch: 2670, Train Loss: 0.573, Val AUC: 0.531\n",
      "Epoch: 2680, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2690, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 2700, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2710, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 2720, Train Loss: 0.570, Val AUC: 0.533\n",
      "Epoch: 2730, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2740, Train Loss: 0.573, Val AUC: 0.532\n",
      "Epoch: 2750, Train Loss: 0.571, Val AUC: 0.532\n",
      "Epoch: 2760, Train Loss: 0.568, Val AUC: 0.533\n",
      "Epoch: 2770, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 2780, Train Loss: 0.570, Val AUC: 0.533\n",
      "Epoch: 2790, Train Loss: 0.568, Val AUC: 0.532\n",
      "Epoch: 2800, Train Loss: 0.576, Val AUC: 0.531\n",
      "Epoch: 2810, Train Loss: 0.571, Val AUC: 0.532\n",
      "Epoch: 2820, Train Loss: 0.570, Val AUC: 0.532\n",
      "Epoch: 2830, Train Loss: 0.575, Val AUC: 0.533\n",
      "Epoch: 2840, Train Loss: 0.570, Val AUC: 0.532\n",
      "Epoch: 2850, Train Loss: 0.575, Val AUC: 0.533\n",
      "Epoch: 2860, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 2870, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 2880, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 2890, Train Loss: 0.568, Val AUC: 0.533\n",
      "Epoch: 2900, Train Loss: 0.567, Val AUC: 0.533\n",
      "Epoch: 2910, Train Loss: 0.573, Val AUC: 0.536\n",
      "Epoch: 2920, Train Loss: 0.574, Val AUC: 0.533\n",
      "Epoch: 2930, Train Loss: 0.570, Val AUC: 0.533\n",
      "Epoch: 2940, Train Loss: 0.572, Val AUC: 0.531\n",
      "Epoch: 2950, Train Loss: 0.574, Val AUC: 0.533\n",
      "Epoch: 2960, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 2970, Train Loss: 0.573, Val AUC: 0.533\n",
      "Epoch: 2980, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 2990, Train Loss: 0.574, Val AUC: 0.534\n",
      "Epoch: 3000, Train Loss: 0.573, Val AUC: 0.533\n",
      "Epoch: 3010, Train Loss: 0.573, Val AUC: 0.533\n",
      "Epoch: 3020, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 3030, Train Loss: 0.569, Val AUC: 0.534\n",
      "Epoch: 3040, Train Loss: 0.574, Val AUC: 0.532\n",
      "Epoch: 3050, Train Loss: 0.572, Val AUC: 0.534\n",
      "Epoch: 3060, Train Loss: 0.574, Val AUC: 0.533\n",
      "Epoch: 3070, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 3080, Train Loss: 0.576, Val AUC: 0.531\n",
      "Epoch: 3090, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 3100, Train Loss: 0.579, Val AUC: 0.531\n",
      "Epoch: 3110, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 3120, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 3130, Train Loss: 0.571, Val AUC: 0.534\n",
      "Epoch: 3140, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 3150, Train Loss: 0.569, Val AUC: 0.534\n",
      "Epoch: 3160, Train Loss: 0.574, Val AUC: 0.531\n",
      "Epoch: 3170, Train Loss: 0.565, Val AUC: 0.534\n",
      "Epoch: 3180, Train Loss: 0.570, Val AUC: 0.531\n",
      "Epoch: 3190, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 3200, Train Loss: 0.572, Val AUC: 0.533\n",
      "Epoch: 3210, Train Loss: 0.569, Val AUC: 0.534\n",
      "Epoch: 3220, Train Loss: 0.575, Val AUC: 0.535\n",
      "Epoch: 3230, Train Loss: 0.572, Val AUC: 0.534\n",
      "Epoch: 3240, Train Loss: 0.565, Val AUC: 0.536\n",
      "Epoch: 3250, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 3260, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 3270, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 3280, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 3290, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 3300, Train Loss: 0.571, Val AUC: 0.531\n",
      "Epoch: 3310, Train Loss: 0.564, Val AUC: 0.536\n",
      "Epoch: 3320, Train Loss: 0.567, Val AUC: 0.536\n",
      "Epoch: 3330, Train Loss: 0.566, Val AUC: 0.532\n",
      "Epoch: 3340, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 3350, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 3360, Train Loss: 0.571, Val AUC: 0.534\n",
      "Epoch: 3370, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 3380, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 3390, Train Loss: 0.572, Val AUC: 0.534\n",
      "Epoch: 3400, Train Loss: 0.565, Val AUC: 0.534\n",
      "Epoch: 3410, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 3420, Train Loss: 0.568, Val AUC: 0.533\n",
      "Epoch: 3430, Train Loss: 0.570, Val AUC: 0.535\n",
      "Epoch: 3440, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 3450, Train Loss: 0.574, Val AUC: 0.536\n",
      "Epoch: 3460, Train Loss: 0.566, Val AUC: 0.533\n",
      "Epoch: 3470, Train Loss: 0.568, Val AUC: 0.535\n",
      "Epoch: 3480, Train Loss: 0.573, Val AUC: 0.530\n",
      "Epoch: 3490, Train Loss: 0.562, Val AUC: 0.537\n",
      "Epoch: 3500, Train Loss: 0.568, Val AUC: 0.532\n",
      "Epoch: 3510, Train Loss: 0.564, Val AUC: 0.532\n",
      "Epoch: 3520, Train Loss: 0.572, Val AUC: 0.533\n",
      "Epoch: 3530, Train Loss: 0.576, Val AUC: 0.530\n",
      "Epoch: 3540, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 3550, Train Loss: 0.573, Val AUC: 0.529\n",
      "Epoch: 3560, Train Loss: 0.571, Val AUC: 0.534\n",
      "Epoch: 3570, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 3580, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 3590, Train Loss: 0.571, Val AUC: 0.535\n",
      "Epoch: 3600, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 3610, Train Loss: 0.568, Val AUC: 0.533\n",
      "Epoch: 3620, Train Loss: 0.566, Val AUC: 0.533\n",
      "Epoch: 3630, Train Loss: 0.565, Val AUC: 0.535\n",
      "Epoch: 3640, Train Loss: 0.572, Val AUC: 0.535\n",
      "Epoch: 3650, Train Loss: 0.568, Val AUC: 0.535\n",
      "Epoch: 3660, Train Loss: 0.572, Val AUC: 0.536\n",
      "Epoch: 3670, Train Loss: 0.565, Val AUC: 0.535\n",
      "Epoch: 3680, Train Loss: 0.567, Val AUC: 0.534\n",
      "Epoch: 3690, Train Loss: 0.566, Val AUC: 0.535\n",
      "Epoch: 3700, Train Loss: 0.572, Val AUC: 0.536\n",
      "Epoch: 3710, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 3720, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 3730, Train Loss: 0.575, Val AUC: 0.535\n",
      "Epoch: 3740, Train Loss: 0.566, Val AUC: 0.533\n",
      "Epoch: 3750, Train Loss: 0.567, Val AUC: 0.537\n",
      "Epoch: 3760, Train Loss: 0.567, Val AUC: 0.533\n",
      "Epoch: 3770, Train Loss: 0.567, Val AUC: 0.537\n",
      "Epoch: 3780, Train Loss: 0.566, Val AUC: 0.535\n",
      "Epoch: 3790, Train Loss: 0.570, Val AUC: 0.536\n",
      "Epoch: 3800, Train Loss: 0.567, Val AUC: 0.534\n",
      "Epoch: 3810, Train Loss: 0.570, Val AUC: 0.537\n",
      "Epoch: 3820, Train Loss: 0.569, Val AUC: 0.537\n",
      "Epoch: 3830, Train Loss: 0.563, Val AUC: 0.537\n",
      "Epoch: 3840, Train Loss: 0.568, Val AUC: 0.533\n",
      "Epoch: 3850, Train Loss: 0.564, Val AUC: 0.539\n",
      "Epoch: 3860, Train Loss: 0.565, Val AUC: 0.534\n",
      "Epoch: 3870, Train Loss: 0.573, Val AUC: 0.535\n",
      "Epoch: 3880, Train Loss: 0.571, Val AUC: 0.534\n",
      "Epoch: 3890, Train Loss: 0.566, Val AUC: 0.533\n",
      "Epoch: 3900, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 3910, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 3920, Train Loss: 0.569, Val AUC: 0.534\n",
      "Epoch: 3930, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 3940, Train Loss: 0.569, Val AUC: 0.535\n",
      "Epoch: 3950, Train Loss: 0.567, Val AUC: 0.537\n",
      "Epoch: 3960, Train Loss: 0.568, Val AUC: 0.538\n",
      "Epoch: 3970, Train Loss: 0.566, Val AUC: 0.538\n",
      "Epoch: 3980, Train Loss: 0.563, Val AUC: 0.536\n",
      "Epoch: 3990, Train Loss: 0.567, Val AUC: 0.537\n",
      "Epoch: 4000, Train Loss: 0.564, Val AUC: 0.535\n",
      "Epoch: 4010, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 4020, Train Loss: 0.569, Val AUC: 0.534\n",
      "Epoch: 4030, Train Loss: 0.573, Val AUC: 0.535\n",
      "Epoch: 4040, Train Loss: 0.569, Val AUC: 0.535\n",
      "Epoch: 4050, Train Loss: 0.571, Val AUC: 0.536\n",
      "Epoch: 4060, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 4070, Train Loss: 0.570, Val AUC: 0.536\n",
      "Epoch: 4080, Train Loss: 0.566, Val AUC: 0.536\n",
      "Epoch: 4090, Train Loss: 0.570, Val AUC: 0.535\n",
      "Epoch: 4100, Train Loss: 0.571, Val AUC: 0.535\n",
      "Epoch: 4110, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 4120, Train Loss: 0.565, Val AUC: 0.537\n",
      "Epoch: 4130, Train Loss: 0.564, Val AUC: 0.534\n",
      "Epoch: 4140, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 4150, Train Loss: 0.573, Val AUC: 0.534\n",
      "Epoch: 4160, Train Loss: 0.567, Val AUC: 0.536\n",
      "Epoch: 4170, Train Loss: 0.566, Val AUC: 0.536\n",
      "Epoch: 4180, Train Loss: 0.565, Val AUC: 0.534\n",
      "Epoch: 4190, Train Loss: 0.574, Val AUC: 0.536\n",
      "Epoch: 4200, Train Loss: 0.565, Val AUC: 0.535\n",
      "Epoch: 4210, Train Loss: 0.571, Val AUC: 0.535\n",
      "Epoch: 4220, Train Loss: 0.571, Val AUC: 0.534\n",
      "Epoch: 4230, Train Loss: 0.565, Val AUC: 0.536\n",
      "Epoch: 4240, Train Loss: 0.565, Val AUC: 0.535\n",
      "Epoch: 4250, Train Loss: 0.562, Val AUC: 0.537\n",
      "Epoch: 4260, Train Loss: 0.567, Val AUC: 0.537\n",
      "Epoch: 4270, Train Loss: 0.568, Val AUC: 0.535\n",
      "Epoch: 4280, Train Loss: 0.574, Val AUC: 0.536\n",
      "Epoch: 4290, Train Loss: 0.570, Val AUC: 0.535\n",
      "Epoch: 4300, Train Loss: 0.565, Val AUC: 0.534\n",
      "Epoch: 4310, Train Loss: 0.572, Val AUC: 0.535\n",
      "Epoch: 4320, Train Loss: 0.571, Val AUC: 0.535\n",
      "Epoch: 4330, Train Loss: 0.566, Val AUC: 0.533\n",
      "Epoch: 4340, Train Loss: 0.569, Val AUC: 0.537\n",
      "Epoch: 4350, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 4360, Train Loss: 0.572, Val AUC: 0.537\n",
      "Epoch: 4370, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 4380, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 4390, Train Loss: 0.568, Val AUC: 0.532\n",
      "Epoch: 4400, Train Loss: 0.569, Val AUC: 0.536\n",
      "Epoch: 4410, Train Loss: 0.570, Val AUC: 0.533\n",
      "Epoch: 4420, Train Loss: 0.570, Val AUC: 0.534\n",
      "Epoch: 4430, Train Loss: 0.566, Val AUC: 0.535\n",
      "Epoch: 4440, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 4450, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 4460, Train Loss: 0.567, Val AUC: 0.536\n",
      "Epoch: 4470, Train Loss: 0.568, Val AUC: 0.535\n",
      "Epoch: 4480, Train Loss: 0.569, Val AUC: 0.533\n",
      "Epoch: 4490, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 4500, Train Loss: 0.571, Val AUC: 0.531\n",
      "Epoch: 4510, Train Loss: 0.566, Val AUC: 0.532\n",
      "Epoch: 4520, Train Loss: 0.570, Val AUC: 0.535\n",
      "Epoch: 4530, Train Loss: 0.566, Val AUC: 0.532\n",
      "Epoch: 4540, Train Loss: 0.568, Val AUC: 0.538\n",
      "Epoch: 4550, Train Loss: 0.570, Val AUC: 0.533\n",
      "Epoch: 4560, Train Loss: 0.568, Val AUC: 0.537\n",
      "Epoch: 4570, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 4580, Train Loss: 0.565, Val AUC: 0.532\n",
      "Epoch: 4590, Train Loss: 0.572, Val AUC: 0.533\n",
      "Epoch: 4600, Train Loss: 0.565, Val AUC: 0.533\n",
      "Epoch: 4610, Train Loss: 0.564, Val AUC: 0.534\n",
      "Epoch: 4620, Train Loss: 0.565, Val AUC: 0.531\n",
      "Epoch: 4630, Train Loss: 0.567, Val AUC: 0.533\n",
      "Epoch: 4640, Train Loss: 0.567, Val AUC: 0.534\n",
      "Epoch: 4650, Train Loss: 0.564, Val AUC: 0.531\n",
      "Epoch: 4660, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 4670, Train Loss: 0.569, Val AUC: 0.532\n",
      "Epoch: 4680, Train Loss: 0.564, Val AUC: 0.534\n",
      "Epoch: 4690, Train Loss: 0.566, Val AUC: 0.535\n",
      "Epoch: 4700, Train Loss: 0.566, Val AUC: 0.531\n",
      "Epoch: 4710, Train Loss: 0.566, Val AUC: 0.538\n",
      "Epoch: 4720, Train Loss: 0.562, Val AUC: 0.532\n",
      "Epoch: 4730, Train Loss: 0.563, Val AUC: 0.536\n",
      "Epoch: 4740, Train Loss: 0.572, Val AUC: 0.533\n",
      "Epoch: 4750, Train Loss: 0.568, Val AUC: 0.535\n",
      "Epoch: 4760, Train Loss: 0.567, Val AUC: 0.535\n",
      "Epoch: 4770, Train Loss: 0.570, Val AUC: 0.535\n",
      "Epoch: 4780, Train Loss: 0.565, Val AUC: 0.533\n",
      "Epoch: 4790, Train Loss: 0.567, Val AUC: 0.537\n",
      "Epoch: 4800, Train Loss: 0.566, Val AUC: 0.533\n",
      "Epoch: 4810, Train Loss: 0.565, Val AUC: 0.535\n",
      "Epoch: 4820, Train Loss: 0.568, Val AUC: 0.532\n",
      "Epoch: 4830, Train Loss: 0.571, Val AUC: 0.536\n",
      "Epoch: 4840, Train Loss: 0.574, Val AUC: 0.534\n",
      "Epoch: 4850, Train Loss: 0.571, Val AUC: 0.533\n",
      "Epoch: 4860, Train Loss: 0.562, Val AUC: 0.534\n",
      "Epoch: 4870, Train Loss: 0.563, Val AUC: 0.531\n",
      "Epoch: 4880, Train Loss: 0.566, Val AUC: 0.532\n",
      "Epoch: 4890, Train Loss: 0.569, Val AUC: 0.531\n",
      "Epoch: 4900, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 4910, Train Loss: 0.568, Val AUC: 0.534\n",
      "Epoch: 4920, Train Loss: 0.566, Val AUC: 0.534\n",
      "Epoch: 4930, Train Loss: 0.571, Val AUC: 0.532\n",
      "Epoch: 4940, Train Loss: 0.564, Val AUC: 0.533\n",
      "Epoch: 4950, Train Loss: 0.565, Val AUC: 0.532\n",
      "Epoch: 4960, Train Loss: 0.566, Val AUC: 0.532\n",
      "Epoch: 4970, Train Loss: 0.563, Val AUC: 0.532\n",
      "Epoch: 4980, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 4990, Train Loss: 0.572, Val AUC: 0.532\n",
      "Epoch: 5000, Train Loss: 0.568, Val AUC: 0.536\n"
     ]
    }
   ],
   "source": [
    "model = Net(G.x.shape[1], 64, 2).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = train_link_predictor(model, train_data, val_data, optimizer, criterion,n_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test = []\n",
    "target_test = []\n",
    "for i in range(test.shape[0]):\n",
    "    s_new = node_idx.loc[node_idx['old_index'] == np.array(test['Source'])[i], 'new_index'].values[0]\n",
    "    t_new = node_idx.loc[node_idx['old_index'] == np.array(test['Target'])[i], 'new_index'].values[0]\n",
    "    source_test.append(s_new)\n",
    "    target_test.append(t_new)\n",
    "\n",
    "source_test = torch.from_numpy(np.array(source_test))\n",
    "target_test = torch.from_numpy(np.array(target_test))\n",
    "edge_index_test = torch.stack([source_test, target_test], dim=0)\n",
    "assert edge_index_test.size() == (2,3498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3597, 932], edge_index=[2, 3498], edge_label=[3498], edge_label_index=[2, 3498])\n"
     ]
    }
   ],
   "source": [
    "# Create the graph by inserting the data and features directly\n",
    "G_test = Data(x=node_feat, edge_index=edge_index_test, edge_label = torch.zeros(3498), edge_label_index = edge_index_test)\n",
    "print(G_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_link_predictor(model, data):\n",
    "\n",
    "    model.eval()\n",
    "    z = model.encode(data.x.to(device), data.edge_index.to(device))\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    median = np.median(out.cpu().numpy())\n",
    "    preds = [0 if i < median else 1 for i in out.cpu().numpy() ]\n",
    "\n",
    "    return out, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba, preds = test_link_predictor(model,G_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['ID'] = list(range(len(preds)))\n",
    "submission['Predicted'] = preds\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
